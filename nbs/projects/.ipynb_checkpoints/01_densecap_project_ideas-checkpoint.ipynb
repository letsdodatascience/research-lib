{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://arxiv.org/pdf/1412.2306v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to\n",
    "learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a\n",
    "novel combination of Convolutional Neural Networks over\n",
    "image regions, bidirectional Recurrent Neural Networks\n",
    "over sentences, and a structured objective that aligns the\n",
    "two modalities through a multimodal embedding. We then\n",
    "describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate\n",
    "novel descriptions of image regions.\n",
    "\n",
    "\n",
    "**Follow upwork**\n",
    "\n",
    "- [DenseCap: Fully Convolutional Localization Networks for Dense Captioning](https://cs.stanford.edu/people/karpathy/densecap.pdf)\n",
    "\n",
    "    - Implementation: https://github.com/jcjohnson/densecap\n",
    "\n",
    "- [Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge](https://arxiv.org/pdf/1609.06647.pdf)\n",
    "\n",
    "- [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)\n",
    "\n",
    "Intersting Links \n",
    "\n",
    "1. https://blog.heuritech.com/2016/04/15/knowledge-extraction-from-unstructured-texts/\n",
    "2. https://blog.heuritech.com/2018/03/12/why-computer-vision-apis-wont-do-the-trick-for-verticalized-applications-heuritechs-take-in-fashion/\n",
    "3. https://blog.heuritech.com/2016/01/20/attention-mechanism/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "We will build a model:\n",
    "    - generates natural language descriptions of images and their regions\n",
    "        - datasets of images abd their sentence description\n",
    "        - inter-modal correspondences between language and visual data\n",
    "        - cnns over image regions\n",
    "        - biDir-LSTM over sentences\n",
    "        - Structured objective that aligns the two modalities through a multimodal embedding\n",
    "        - Then, \n",
    "            - Multi-modal rnn architecture that uses the inferred alignments to lean to generate novel description of image regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset / Databunch\n",
    "- Models\n",
    "    - [Deep Fragment Embeddings for Bidirectional Image Sentence Mapping](https://arxiv.org/pdf/1406.5679.pdf)\n",
    "- Loss Function\n",
    "- Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flickr8K, Flickr30K and MSCOCO datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[visualgenome dataset](https://visualgenome.org/) used for their final work DenseCap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./../images/densecap_demo.png\" alt=\"Resnet\" width=\"1050\" height=\"650\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link to the main paper that started the interst in this project is [Deep Visual-Semantic Alignments for Generating Image Descriptions (2015)](https://arxiv.org/pdf/1412.2306v2.pdf) by Karpathy. In what seems to be a predecessor work in [Deep Fragment Embeddings for Bidirectional Image Sentence Mapping (2014)](https://arxiv.org/pdf/1406.5679.pdf), they have certain ideas to the main paper concept. now, in what seems to be a successor paer to our main paper is [DenseCap: Fully Convolutional Localization Networks for Dense Captioning]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-fastai",
   "language": "python",
   "name": "my-fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
