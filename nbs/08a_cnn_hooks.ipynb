{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lib.nb_07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function lib.nb_01.normalize(x, m, s)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize_to(train, valid):\n",
    "    m,s = train.mean(),train.std()\n",
    "    return normalize(train, m, s), normalize(valid, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_valid = normalize_to(x_train,x_valid)\n",
    "train_ds,valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh,bs = 50,512\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c_out=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Lambda(nn.Module):\n",
    "    \n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.func(x)\n",
    "    \n",
    "def flatten(x): return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mnist_resize(x): return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model(data):\n",
    "    return nn.Sequential(\n",
    "        Lambda(mnist_resize),\n",
    "        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14\n",
    "        nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7\n",
    "        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n",
    "        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        Lambda(flatten),\n",
    "        nn.Linear(32,data.c_out)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cos_1cycle_anneal(start, high, end):\n",
    "    return [sched_cos(start, high), sched_cos(high, end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data)\n",
    "\n",
    "\n",
    "cbs = [partial(AvgStatsCallback,accuracy),\n",
    "       partial(CudaCallback, get_device()), \n",
    "       Recorder,\n",
    "       #partial(SaveModelCallback, every=\"improvement\", savename=\"basic_seq2seq_model\"),\n",
    "       #partial(GradientClipping, clip=0.1),\n",
    "       ProgressCallback]\n",
    "\n",
    "lr = 1e-2\n",
    "\n",
    "sched_lr  = combine_scheds([0.3,0.7], cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "sched_mom = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.8, 0.7, 0.8))\n",
    "cbsched = [ParamScheduler('lr', sched_lr) , ParamScheduler('mom', sched_mom)]\n",
    "\n",
    "\n",
    "learn = Learner(model, data, loss_func=loss_func, cb_funcs=cbs, opt_func=adam_opt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.864158</td>\n",
       "      <td>0.706220</td>\n",
       "      <td>0.204743</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.146040</td>\n",
       "      <td>0.955820</td>\n",
       "      <td>0.103797</td>\n",
       "      <td>0.968700</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.079235</td>\n",
       "      <td>0.976300</td>\n",
       "      <td>0.073493</td>\n",
       "      <td>0.978400</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(3, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refactor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv_layer(ni, nf, ks=3, stride=2):\n",
    "    return nn.Sequential(nn.Conv2d(ni,nf,ks, padding=ks//2, stride=stride), nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model should be independent of dataset specific resizing etc, we can do resizing in a callback before feeding in the data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BatchTransformXCallback(Callback):\n",
    "    _order = 2 #important to keep track of orders as time goes on and we create more callbacks\n",
    "    \n",
    "    def __init__(self, tfm): self.tfm = tfm\n",
    "    def begin_batch(self): self.run.xb = self.tfm(self.run.xb)\n",
    "        \n",
    "def view_tfm(*size):\n",
    "    def _inner(x): return x.view(*((-1,)+size))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_view = view_tfm(1,28,28)\n",
    "cbs.append(partial(BatchTransformXCallback, mnist_view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [8,16,32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_layers(data, nfs, conv_layer=conv_layer):\n",
    "    nfs = [1] + nfs\n",
    "    return [conv_layer(nfs[i],nfs[i+1], 5 if i==0 else 3) for i in range(len(nfs)-1)] + [\n",
    "        nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c_out)\n",
    "    ]\n",
    "\n",
    "def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data,nfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): AdaptiveAvgPool2d(output_size=1)\n",
       "  (5): Lambda()\n",
       "  (6): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cnn_model(data,nfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_cnn_model() got an unexpected keyword argument 'conv_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-33e5fe00e923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m cbs = [partial(AvgStatsCallback,accuracy),\n\u001b[1;32m      5\u001b[0m        \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCudaCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_cnn_model() got an unexpected keyword argument 'conv_layer'"
     ]
    }
   ],
   "source": [
    "model = get_cnn_model(data, nfs, conv_layer=conv_layer)\n",
    "\n",
    "\n",
    "cbs = [partial(AvgStatsCallback,accuracy),\n",
    "       partial(CudaCallback, get_device()), \n",
    "       Recorder,\n",
    "       partial(BatchTransformXCallback, mnist_resize),\n",
    "       #partial(SaveModelCallback, every=\"improvement\", savename=\"basic_seq2seq_model\"),\n",
    "       #partial(GradientClipping, clip=0.1),\n",
    "       ProgressCallback]\n",
    "\n",
    "lr = 1e-2\n",
    "\n",
    "sched_lr  = combine_scheds([0.3,0.7], cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "sched_mom = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.8, 0.7, 0.8))\n",
    "cbsched = [ParamScheduler('lr', sched_lr) , ParamScheduler('mom', sched_mom)]\n",
    "\n",
    "\n",
    "learn = Learner(model, data, loss_func=loss_func, cb_funcs=cbs, opt_func=adam_opt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.096784</td>\n",
       "      <td>0.970180</td>\n",
       "      <td>0.101155</td>\n",
       "      <td>0.969700</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.072458</td>\n",
       "      <td>0.977620</td>\n",
       "      <td>0.077027</td>\n",
       "      <td>0.977400</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.039524</td>\n",
       "      <td>0.988460</td>\n",
       "      <td>0.055958</td>\n",
       "      <td>0.984700</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(3, cbs = cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_basic_cnn_learner(model, data, loss_func, opt_func=adam_opt(), lr=1e-2):\n",
    "    cbs = [partial(AvgStatsCallback,accuracy),\n",
    "       partial(CudaCallback, get_device()), \n",
    "       Recorder,\n",
    "       partial(BatchTransformXCallback, mnist_resize),\n",
    "       #partial(SaveModelCallback, every=\"improvement\", savename=\"basic_seq2seq_model\"),\n",
    "       #partial(GradientClipping, clip=0.1),\n",
    "       ProgressCallback]\n",
    "    \n",
    "    lr = lr\n",
    "\n",
    "    sched_lr  = combine_scheds([0.3,0.7], cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "    sched_mom = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.8, 0.7, 0.8))\n",
    "    cbsched = [ParamScheduler('lr', sched_lr) , ParamScheduler('mom', sched_mom)]\n",
    "    \n",
    "    return Learner(model, data, loss_func=loss_func, cb_funcs=cbs, opt_func=adam_opt()), cbsched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs, conv_layer=conv_layer)\n",
    "learn, cbsched = get_basic_cnn_learner(model, data, F.cross_entropy, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(3, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do some telemetry, and want the mean and standard deviation of each activations in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooks are PyTorch object you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook).\n",
    "\n",
    "Hooks don't require us to rewrite the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data,nfs, conv_layer=conv_layer)\n",
    "learn, cbsched = get_basic_cnn_learner(model, data, F.cross_entropy, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_means = [[] for _ in model]\n",
    "act_stds = [[] for _ in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(i, mod, inp, outp):\n",
    "    if mod.training:\n",
    "        act_means[i].append(outp.data.mean())\n",
    "        act_stds[i].append(outp.data.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in act_means: plt.plot(o)\n",
    "plt.legend(range(5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(act_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hooks Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def children(m):\n",
    "    return list(m.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Hook():\n",
    "    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook, 'stats'): hook.stats = ([],[])\n",
    "    means, stds = hook.stats\n",
    "    if mod.training:\n",
    "        means.append(outp.data.mean())\n",
    "        stds.append(outp.data.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data,nfs, conv_layer=conv_layer)\n",
    "learn, cbsched = get_basic_cnn_learner(model, data, F.cross_entropy, lr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = [Hook(l, append_stats) for l in children(model[:4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hooks:\n",
    "    plt.plot(h.stats[1])\n",
    "    h.remove()\n",
    "plt.legend(range(4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Hooks(ListContainer):\n",
    "    \n",
    "    def __init__(self, ms, f): super().__init__([Hook(m,f) for m in ms])\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__(self, *args): self.remove()\n",
    "    def __del__(self): self.remove()\n",
    "    \n",
    "    def __delitem__(self,i):\n",
    "        self[i].remove()\n",
    "        super().__delitem__(i)\n",
    "        \n",
    "    def remove(self):\n",
    "        for h in self: h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data,nfs, conv_layer=conv_layer)\n",
    "learn, cbsched = get_basic_cnn_learner(model, data, F.cross_entropy, lr=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with `__enter__` and `__exit__`, we can usee `Hooks` within a context so that python takes care of removing the unused hooks after we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(learn.model, append_stats) as hooks:\n",
    "    \n",
    "    learn.fit(2)\n",
    "    \n",
    "    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "    for h in hooks:\n",
    "        ms,ss = h.stats\n",
    "        ax0.plot(ms[:10])\n",
    "        ax1.plot(ss[:10])\n",
    "    plt.legend(range(6));\n",
    "    \n",
    "    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "    for h in hooks:\n",
    "        ms,ss = h.stats\n",
    "        ax0.plot(ms)\n",
    "        ax1.plot(ss)\n",
    "    plt.legend(range(6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data,nfs, conv_layer=conv_layer)\n",
    "learn, cbsched = get_basic_cnn_learner(model, data, F.cross_entropy, lr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model:\n",
    "    if isinstance(l, nn.Sequential):\n",
    "        init.kaiming_normal_(l[0].weight)\n",
    "        l[0].bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(learn.model, append_stats) as hooks:\n",
    "    \n",
    "    learn.fit(2)\n",
    "    \n",
    "    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "    for h in hooks:\n",
    "        ms,ss = h.stats\n",
    "        ax0.plot(ms[:10])\n",
    "        ax1.plot(ss[:10])\n",
    "    plt.legend(range(6));\n",
    "    \n",
    "    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "    for h in hooks:\n",
    "        ms,ss = h.stats\n",
    "        ax0.plot(ms)\n",
    "        ax1.plot(ss)\n",
    "    plt.legend(range(6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "    means,stds,hists = hook.stats\n",
    "    if mod.training:\n",
    "        means.append(outp.data.mean().cpu())\n",
    "        stds .append(outp.data.std().cpu())\n",
    "        hists.append(outp.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data,nfs, conv_layer=conv_layer)\n",
    "learn, cbsched = get_basic_cnn_learner(model, data, F.cross_entropy, lr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model:\n",
    "    if isinstance(l, nn.Sequential):\n",
    "        init.kaiming_normal_(l[0].weight)\n",
    "        l[0].bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(learn.model, append_stats) as hooks: learn.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"image.cmap\"] = \"viridis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to @ste for initial version of histogram plotting code\n",
    "def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,2, figsize=(15,6))\n",
    "for ax,h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.imshow(get_hist(h), origin='lower')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(h):\n",
    "    h1 = torch.stack(h.stats[2]).t().float()\n",
    "    return h1[:2].sum(0)/h1.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,2, figsize=(15,6))\n",
    "for ax,h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cnn_layers(data, nfs, layer, **kwargs):\n",
    "    nfs = [1] + nfs\n",
    "    return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs)\n",
    "            for i in range(len(nfs)-1)] + [\n",
    "        nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c_out)]\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=2, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), GeneralRelu(**kwargs))\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x.sub_(self.sub)\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "\n",
    "def init_cnn(m, uniform=False):\n",
    "    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_\n",
    "    for l in m:\n",
    "        if isinstance(l, nn.Sequential):\n",
    "            f(l[0].weight, a=0.1)\n",
    "            l[0].bias.data.zero_()\n",
    "\n",
    "def get_cnn_model(data, nfs, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "    means,stds,hists = hook.stats\n",
    "    if mod.training:\n",
    "        means.append(outp.data.mean().cpu())\n",
    "        stds .append(outp.data.std().cpu())\n",
    "        hists.append(outp.data.cpu().histc(40,-7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  get_cnn_model(data, nfs, conv_layer, leak=0.1, sub=0.4, maxv=6.)\n",
    "init_cnn(model, uniform=True)\n",
    "learn, cbsched = get_basic_cnn_learner(model, data, F.cross_entropy, lr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(model, append_stats) as hooks:\n",
    "    learn.fit(1)\n",
    "    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "    for h in hooks:\n",
    "        ms,ss,hi = h.stats\n",
    "        ax0.plot(ms[:10])\n",
    "        ax1.plot(ss[:10])\n",
    "    plt.legend(range(5));\n",
    "    \n",
    "    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "    for h in hooks:\n",
    "        ms,ss,hi = h.stats\n",
    "        ax0.plot(ms)\n",
    "        ax1.plot(ss)\n",
    "    plt.legend(range(5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,2, figsize=(15,6))\n",
    "for ax,h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.imshow(get_hist(h), origin='lower')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(h):\n",
    "    h1 = torch.stack(h.stats[2]).t().float()\n",
    "    return h1[19:22].sum(0)/h1.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,2, figsize=(15,6))\n",
    "for ax,h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 notebook2script.py 08a_cnn_hooks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-fastai",
   "language": "python",
   "name": "my-fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
