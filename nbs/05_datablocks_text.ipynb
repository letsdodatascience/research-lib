{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lib.nb_04 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
    "- https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA\n",
    "- https://arxiv.org/pdf/1904.10324.pdf\n",
    "- https://www.youtube.com/watch?v=5vcj8kSwBCY&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=15&t=2416s\n",
    "- https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html\n",
    "- https://www.youtube.com/watch?v=qzljG6DKgic&feature=youtu.be\n",
    "- https://arxiv.org/pdf/1904.11272.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)\n",
    "tfms = [MakeRGB(), ResizeFixed(128), to_byte_tensor, to_float_tensor]\n",
    "\n",
    "il = ImageList.from_files(path, tfms=tfms)\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\n",
    "data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledData\n",
       "x: ImageList (12894 items)\n",
       "[PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_58454.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_32588.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_32422.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_33663.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_27948.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_38644.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_35076.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_38214.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_23008.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/train/n03394916/n03394916_33008.JPEG')...]\n",
       "Path: /Users/vks/.fastai/data/imagenette-160\n",
       "y: ItemList (12894 items)\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n",
       "Path: /Users/vks/.fastai/data/imagenette-160"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledData\n",
       "x: ImageList (500 items)\n",
       "[PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00046669.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00033682.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00005548.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00028723.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00048303.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00048138.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00025761.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00017407.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00035433.JPEG'), PosixPath('/Users/vks/.fastai/data/imagenette-160/val/n03394916/ILSVRC2012_val_00041247.JPEG')...]\n",
       "Path: /Users/vks/.fastai/data/imagenette-160\n",
       "y: ItemList (500 items)\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n",
       "Path: /Users/vks/.fastai/data/imagenette-160"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text ItemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/vks/.fastai/data/imdb/ld.pkl'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/test'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/ll_lm.pkl'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/tmp_clas'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/vocab_lm.pkl'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/README'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/ll_clas.pkl'),\n",
       " PosixPath('/Users/vks/.fastai/data/imdb/train')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_file(fn): \n",
    "    with open(fn, 'r', encoding = 'utf8') as f: return f.read()\n",
    "    \n",
    "class TextList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):\n",
    "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "    \n",
    "    def get(self, i):\n",
    "        if isinstance(i, Path): return read_file(i)\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(il.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextList (100000 items)\n",
       "[PosixPath('/Users/vks/.fastai/data/imdb/test/neg/1821_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/9487_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/4604_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/2828_2.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/10890_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/3351_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/8070_2.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/1027_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/8248_3.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/4290_4.txt')...]\n",
       "Path: /Users/vks/.fastai/data/imdb"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subspecies is set in Romania where two American college students Michele (Laura Mae Tate) & Lillian (Michelle McBride) arrive to study local folklore with the aid of local friend Mara (Irina Movila). There they rent rooms in a hotel & become curious about the mysterious ruins of a nearby castle, it turns out that a powerful & evil Vampire named Radu (Anders Hove) lives there who has stolen the Bloodstone from his father King Vladislav (Angus Scrimm). Radu takes a fancy to the three girls & starts drinking the blood of Mara & Lillian, meanwhile Michele falls for a guy named Stefan (Michael Watson) who just so happens to be Radu's brother. Michele & Stefan decide to team up & rid the world of the evil Radu...<br /><br />Directed by Ted Nicolaou this film seems to be quite highly regarded amongst genre fans & while it's not terrible I certainly wouldn't call it very good & I could't really see anything much to get excited about. Subspecies is a rather slow going film, not that much actually happens & while it does try to stay close to certain classic Vampire lore there's all this nonsense about a Bloodstone & some little monsters that grow from the tips of Radu's severed fingers for some reason. Subspecies could have been a half decent film if not for the fact that it's dull, I really can't remember that much about it, good or bad. The character's are alright but some f the dialogue is silly & there's a scene which bugged me near the start when the girls are at the castle ruins & one says they have to go because it's getting dark yet it's still clearly the middle of the day & very bright. There's also a scene where one of the American girls finds a coffin that hotel's attic & doesn't really seem that bothered by it, I am not being funny but is some bloke whose house I was staying at had a coffin in his attic I would be very, very worried if you know what I mean. I don't think I would ever want to watch it again, there's no real threat, the plot is weak that mixes classic Vampire themes with silly subplots & I was distinctly unmoved by it all. Not the worst film ever but hardly the best either.<br /><br />The film looks alright with nice locations & some local scenery although you feel the look is down to the budget rather than the makers attempt a authenticity. There's not much gore apart from a decapitation & some broken off finger tips. For no apparent reason the makers throw in some average looking stop-motion animated monsters that really don't do anything or have much significance to the story.<br /><br />Filmed on the cheap by Charles Band's Full Moon Entertainment production company in Bucharest in Romania, the production values are alright & better than many later day Band productions. The acting isn't great with many of the cast putting in below par performances while genre regular Angus Scrimm has a small cameo at the start. There's a little bit of style here on occasion with a few scene reminding heavily of the original Nosferatu (1922) in particular the bit showing Radu's shadow coming down the stair with his long claw like fingernails standing out.<br /><br />Subspecies is a film that many seem to like for reasons I don't quite see, I thought it was throughly average at best & overall rather dull. Followed by Bloodstone: Subspecies II (1993), Bloodlust: Subspecies III (1994), Subspecies 4: Bloodstorm (1998) & the spin0off film Vampire Journals (1997).\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = il[20]\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/vks/.fastai/data/imdb/test/neg/2351_4.txt')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "il.items[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def random_splitter(item, p_valid=0.1):\n",
    "    return random.random() < p_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextList (100000 items)\n",
       "[PosixPath('/Users/vks/.fastai/data/imdb/test/neg/1821_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/9487_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/4604_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/2828_2.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/10890_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/3351_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/8070_2.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/1027_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/8248_3.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/4290_4.txt')...]\n",
       "Path: /Users/vks/.fastai/data/imdb"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(il,partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: TextList (90043 items)\n",
       "[PosixPath('/Users/vks/.fastai/data/imdb/test/neg/1821_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/9487_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/4604_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/2828_2.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/10890_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/3351_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/8070_2.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/1027_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/4290_4.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/11890_1.txt')...]\n",
       "Path: /Users/vks/.fastai/data/imdb\n",
       "Valid: TextList (9957 items)\n",
       "[PosixPath('/Users/vks/.fastai/data/imdb/test/neg/8248_3.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/10096_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/2008_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/1789_2.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/9565_3.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/12430_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/12416_2.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/3680_1.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/5988_3.txt'), PosixPath('/Users/vks/.fastai/data/imdb/test/neg/2150_1.txt')...]\n",
       "Path: /Users/vks/.fastai/data/imdb"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Worst movie ever seen. Worst acting too. I cannot imagine a movie worse then this. Nothing to see. No acting at all.T hey (actors) should look for another job. I cant't understand who was stupid enough to actually put money into this movie.<br /><br />I'm sorry for Eric Roberts. Must be tough...I cannot imagine how HUUUGE his mortgage must be to justify taking the job!<br /><br />The ladies in the movie...perhaps they better stick to XXX.<br /><br />As for the LEADING MAN...what a lead! He better be put on a lead and stay there! I can see him being more successful at barking rather than acting. <br /><br />Overall rating: Do NOt rent...DO NOT BUY!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.valid[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize the dataset first, which is splitting a sentence in individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: don't for instance is split between do and n't. We will use a processor for this, in conjunction with the spacy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import spacy,html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even tokenizeing, we will apply a bit of preprocessing on the texts to clean them up (we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#special tokens\n",
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replaces the <br /> by \\n\"\n",
    "    re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "    return re_br.sub(\"\\n\", t)\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return re.sub(r'([/#])', r' \\1 ', t)\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "    \n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n",
    "    def _replace_wrep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fixup_text(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "    \n",
    "default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxrep 4 c '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_rep('cccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxwrep 5 word  '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_wrep('word word word word word ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules are applies after the tokenization on the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n",
    "        else: res.append(t)\n",
    "    return res\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == '': continue\n",
    "        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return res\n",
    "\n",
    "def add_eos_bos(x): return [BOS] + x + [EOS]\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'xxup', 'am', 'xxup', 'shouting']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_all_caps(['I', 'AM', 'SHOUTING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxmaj', 'my', 'name', 'is', 'xxmaj', 'jeremy']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_caps(['My', 'name', 'is', 'Jeremy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from spacy.symbols import ORTH\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel(func, arr, max_workers=4):\n",
    "    if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))\n",
    "    if any([o is not None for o in results]): return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenizeProcessor(Processor):\n",
    "    def __init__(self, lang=\"en\", chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): \n",
    "        self.chunksize,self.max_workers = chunksize,max_workers\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.pre_rules  = default_pre_rules  if pre_rules  is None else pre_rules\n",
    "        self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "\n",
    "    def proc_chunk(self, args):\n",
    "        i,chunk = args\n",
    "        chunk = [compose(t, self.pre_rules) for t in chunk]\n",
    "        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n",
    "        docs = [compose(t, self.post_rules) for t in docs]\n",
    "        return docs\n",
    "\n",
    "    def __call__(self, items): \n",
    "        toks = []\n",
    "        if isinstance(items[0], Path): items = [read_file(i) for i in items]\n",
    "        chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))]\n",
    "        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "        return sum(toks, [])\n",
    "    \n",
    "    def proc1(self, item): return self.proc_chunk([toks])[0]\n",
    "    \n",
    "    def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]\n",
    "    def deproc1(self, tok):    return \" \".join(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TokenizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subspecies is set in Romania where two American college students Michele (Laura Mae Tate) & Lillian (Michelle McBride) arrive to study local folklore with the aid of local friend Mara (Irina Movila). There they rent rooms in a hotel & become curious '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[['xxbos',\n",
       "  'xxmaj',\n",
       "  'alan',\n",
       "  'xxmaj',\n",
       "  'rickman',\n",
       "  '&',\n",
       "  'xxmaj',\n",
       "  'emma',\n",
       "  'xxmaj',\n",
       "  'thompson',\n",
       "  'give',\n",
       "  'good',\n",
       "  'performances',\n",
       "  'with',\n",
       "  'southern',\n",
       "  '/',\n",
       "  'xxmaj',\n",
       "  'new',\n",
       "  'xxmaj',\n",
       "  'orleans',\n",
       "  'accents',\n",
       "  'in',\n",
       "  'this',\n",
       "  'detective',\n",
       "  'flick',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'worth',\n",
       "  'seeing',\n",
       "  'for',\n",
       "  'their',\n",
       "  'scenes-',\n",
       "  'and',\n",
       "  'xxmaj',\n",
       "  'rickman',\n",
       "  \"'s\",\n",
       "  'scene',\n",
       "  'with',\n",
       "  'xxmaj',\n",
       "  'hal',\n",
       "  'xxmaj',\n",
       "  'holbrook',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'these',\n",
       "  'three',\n",
       "  'actors',\n",
       "  'mannage',\n",
       "  'to',\n",
       "  'entertain',\n",
       "  'us',\n",
       "  'no',\n",
       "  'matter',\n",
       "  'what',\n",
       "  'the',\n",
       "  'movie',\n",
       "  ',',\n",
       "  'it',\n",
       "  'seems',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'for',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'shows',\n",
       "  'potential',\n",
       "  ',',\n",
       "  'but',\n",
       "  'one',\n",
       "  'gets',\n",
       "  'the',\n",
       "  'impression',\n",
       "  'in',\n",
       "  'watching',\n",
       "  'the',\n",
       "  'film',\n",
       "  'that',\n",
       "  'it',\n",
       "  'was',\n",
       "  'not',\n",
       "  'pulled',\n",
       "  'off',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'it',\n",
       "  'could',\n",
       "  'have',\n",
       "  'been',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  'cluttered',\n",
       "  'by',\n",
       "  'a',\n",
       "  'rather',\n",
       "  'uninteresting',\n",
       "  'subplot',\n",
       "  'and',\n",
       "  'mostly',\n",
       "  'uninteresting',\n",
       "  'kidnappers',\n",
       "  'really',\n",
       "  'muddles',\n",
       "  'things',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'worth',\n",
       "  'a',\n",
       "  'view-',\n",
       "  'if',\n",
       "  'for',\n",
       "  'nothing',\n",
       "  'more',\n",
       "  'than',\n",
       "  'entertaining',\n",
       "  'performances',\n",
       "  'by',\n",
       "  'xxmaj',\n",
       "  'rickman',\n",
       "  ',',\n",
       "  'xxmaj',\n",
       "  'thompson',\n",
       "  ',',\n",
       "  'and',\n",
       "  'xxmaj',\n",
       "  'holbrook',\n",
       "  '.',\n",
       "  'xxeos'],\n",
       " ['xxbos',\n",
       "  'i',\n",
       "  'have',\n",
       "  'seen',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'and',\n",
       "  'i',\n",
       "  'did',\n",
       "  'not',\n",
       "  'care',\n",
       "  'for',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'anyhow',\n",
       "  '.',\n",
       "  'i',\n",
       "  'would',\n",
       "  'not',\n",
       "  'think',\n",
       "  'about',\n",
       "  'going',\n",
       "  'to',\n",
       "  'xxmaj',\n",
       "  'paris',\n",
       "  'because',\n",
       "  'i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'like',\n",
       "  'this',\n",
       "  'country',\n",
       "  'and',\n",
       "  'its',\n",
       "  'national',\n",
       "  'capital',\n",
       "  '.',\n",
       "  'i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'like',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'french',\n",
       "  'anyhow',\n",
       "  'because',\n",
       "  'i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'understand',\n",
       "  'their',\n",
       "  'language',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'why',\n",
       "  'would',\n",
       "  'i',\n",
       "  'go',\n",
       "  'to',\n",
       "  'xxmaj',\n",
       "  'france',\n",
       "  'when',\n",
       "  'i',\n",
       "  'rather',\n",
       "  'go',\n",
       "  'to',\n",
       "  'xxmaj',\n",
       "  'germany',\n",
       "  'or',\n",
       "  'the',\n",
       "  'xxmaj',\n",
       "  'united',\n",
       "  'xxmaj',\n",
       "  'kingdom',\n",
       "  '?',\n",
       "  'xxmaj',\n",
       "  'germany',\n",
       "  'and',\n",
       "  'the',\n",
       "  'xxmaj',\n",
       "  'united',\n",
       "  'xxmaj',\n",
       "  'kingdom',\n",
       "  'are',\n",
       "  'the',\n",
       "  'nations',\n",
       "  'i',\n",
       "  'tolerate',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'apparently',\n",
       "  'the',\n",
       "  'xxmaj',\n",
       "  'olsen',\n",
       "  'xxmaj',\n",
       "  'twins',\n",
       "  'do',\n",
       "  'not',\n",
       "  'understand',\n",
       "  'the',\n",
       "  'xxmaj',\n",
       "  'french',\n",
       "  'language',\n",
       "  'just',\n",
       "  'like',\n",
       "  'me',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'therefore',\n",
       "  'i',\n",
       "  'will',\n",
       "  'not',\n",
       "  'bother',\n",
       "  'the',\n",
       "  'xxmaj',\n",
       "  'france',\n",
       "  'trip',\n",
       "  'no',\n",
       "  'matter',\n",
       "  'what',\n",
       "  '.',\n",
       "  'i',\n",
       "  'might',\n",
       "  'as',\n",
       "  'well',\n",
       "  'stick',\n",
       "  'to',\n",
       "  'the',\n",
       "  'xxmaj',\n",
       "  'united',\n",
       "  'xxmaj',\n",
       "  'kingdom',\n",
       "  'and',\n",
       "  'meet',\n",
       "  'single',\n",
       "  'women',\n",
       "  'and',\n",
       "  'play',\n",
       "  'video',\n",
       "  'games',\n",
       "  'if',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'video',\n",
       "  'arcade',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'that',\n",
       "  'is',\n",
       "  'all',\n",
       "  '.',\n",
       "  'xxeos'],\n",
       " ['xxbos',\n",
       "  'xxmaj',\n",
       "  'in',\n",
       "  'xxmaj',\n",
       "  'los',\n",
       "  'xxmaj',\n",
       "  'angeles',\n",
       "  ',',\n",
       "  'the',\n",
       "  'alcoholic',\n",
       "  'and',\n",
       "  'lazy',\n",
       "  'xxmaj',\n",
       "  'hank',\n",
       "  'xxmaj',\n",
       "  'chinaski',\n",
       "  '(',\n",
       "  'xxmaj',\n",
       "  'matt',\n",
       "  'xxmaj',\n",
       "  'dillon',\n",
       "  ')',\n",
       "  'performs',\n",
       "  'a',\n",
       "  'wide',\n",
       "  'range',\n",
       "  'of',\n",
       "  'non',\n",
       "  '-',\n",
       "  'qualified',\n",
       "  'functions',\n",
       "  'just',\n",
       "  'to',\n",
       "  'get',\n",
       "  'enough',\n",
       "  'money',\n",
       "  'to',\n",
       "  'drink',\n",
       "  'and',\n",
       "  'gamble',\n",
       "  'in',\n",
       "  'horse',\n",
       "  'races',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'his',\n",
       "  'primary',\n",
       "  'and',\n",
       "  'only',\n",
       "  'objective',\n",
       "  'is',\n",
       "  'writing',\n",
       "  'and',\n",
       "  'having',\n",
       "  'sexy',\n",
       "  'with',\n",
       "  'dirty',\n",
       "  'women',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  '\"',\n",
       "  'xxmaj',\n",
       "  'factotum',\n",
       "  '\"',\n",
       "  'is',\n",
       "  'an',\n",
       "  'uninteresting',\n",
       "  ',',\n",
       "  'pointless',\n",
       "  'and',\n",
       "  'extremely',\n",
       "  'boring',\n",
       "  'movie',\n",
       "  'about',\n",
       "  'an',\n",
       "  'irresponsible',\n",
       "  'drunken',\n",
       "  'vagrant',\n",
       "  'that',\n",
       "  'works',\n",
       "  'a',\n",
       "  'couple',\n",
       "  'of',\n",
       "  'days',\n",
       "  'or',\n",
       "  'weeks',\n",
       "  'just',\n",
       "  'to',\n",
       "  'get',\n",
       "  'enough',\n",
       "  'money',\n",
       "  'to',\n",
       "  'buy',\n",
       "  'spirits',\n",
       "  'and',\n",
       "  'gamble',\n",
       "  ',',\n",
       "  'being',\n",
       "  'immediately',\n",
       "  'fired',\n",
       "  'due',\n",
       "  'to',\n",
       "  'his',\n",
       "  'reckless',\n",
       "  'behavior',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'in',\n",
       "  'accordance',\n",
       "  'with',\n",
       "  'imdb',\n",
       "  ',',\n",
       "  'this',\n",
       "  'character',\n",
       "  'would',\n",
       "  'be',\n",
       "  'the',\n",
       "  'fictional',\n",
       "  'alter',\n",
       "  '-',\n",
       "  'ego',\n",
       "  'of',\n",
       "  'the',\n",
       "  'author',\n",
       "  'xxmaj',\n",
       "  'charles',\n",
       "  'xxmaj',\n",
       "  'bukowski',\n",
       "  ',',\n",
       "  'and',\n",
       "  'based',\n",
       "  'on',\n",
       "  'this',\n",
       "  'story',\n",
       "  ',',\n",
       "  'i',\n",
       "  'will',\n",
       "  'certainly',\n",
       "  'never',\n",
       "  'read',\n",
       "  'any',\n",
       "  'of',\n",
       "  'his',\n",
       "  'novels',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'honestly',\n",
       "  ',',\n",
       "  'if',\n",
       "  'the',\n",
       "  'viewer',\n",
       "  'likes',\n",
       "  'this',\n",
       "  'theme',\n",
       "  'of',\n",
       "  'alcoholic',\n",
       "  'couples',\n",
       "  ',',\n",
       "  'better',\n",
       "  'off',\n",
       "  'watching',\n",
       "  'the',\n",
       "  'touching',\n",
       "  'and',\n",
       "  'heartbreaking',\n",
       "  'xxmaj',\n",
       "  'hector',\n",
       "  'xxmaj',\n",
       "  'babenco',\n",
       "  \"'s\",\n",
       "  '\"',\n",
       "  'xxmaj',\n",
       "  'ironweed',\n",
       "  '\"',\n",
       "  'or',\n",
       "  'xxmaj',\n",
       "  'marco',\n",
       "  'xxmaj',\n",
       "  'ferreri',\n",
       "  \"'s\",\n",
       "  '\"',\n",
       "  'xxmaj',\n",
       "  'storie',\n",
       "  'di',\n",
       "  'xxmaj',\n",
       "  'ordinaria',\n",
       "  'xxmaj',\n",
       "  'follia',\n",
       "  '\"',\n",
       "  'that',\n",
       "  'is',\n",
       "  'based',\n",
       "  'on',\n",
       "  'the',\n",
       "  'life',\n",
       "  'of',\n",
       "  'the',\n",
       "  'same',\n",
       "  'writer',\n",
       "  '.',\n",
       "  'xxmaj',\n",
       "  'my',\n",
       "  'vote',\n",
       "  'is',\n",
       "  'four',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'xxmaj',\n",
       "  'title',\n",
       "  '(',\n",
       "  'xxmaj',\n",
       "  'brazil',\n",
       "  ')',\n",
       "  ':',\n",
       "  '\"',\n",
       "  'xxmaj',\n",
       "  'factotum',\n",
       "  '\\x96',\n",
       "  'xxmaj',\n",
       "  'sem',\n",
       "  'xxmaj',\n",
       "  'destino',\n",
       "  '\"',\n",
       "  '(',\n",
       "  '\"',\n",
       "  'xxmaj',\n",
       "  'factotum',\n",
       "  '\\x96',\n",
       "  'xxmaj',\n",
       "  'without',\n",
       "  'xxmaj',\n",
       "  'destiny',\n",
       "  '\"',\n",
       "  ')',\n",
       "  'xxeos']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp(sd.train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "    def __init__(self, vocab=None, max_vocab=60000, min_freq=2): \n",
    "        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq\n",
    "    \n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            freq = Counter(p for o in items for p in o)\n",
    "            self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c >= self.min_freq]\n",
    "            #insert all the special token in the begining of the vocab\n",
    "            for o in reversed(default_spec_tok):\n",
    "                if o in self.vocab: self.vocab.remove(o)\n",
    "                self.vocab.insert(0, o)\n",
    "        if getattr(self, 'otoi', None) is None:\n",
    "            self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) \n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return [self.otoi[o] for o in item]\n",
    "    \n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    \n",
    "    def deproc1(self, idx): return [self.vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='46' class='' max='46', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [46/46 03:17<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:23<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 11.5 s, total: 40.6 s\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%time ll = label_by_func(sd, lambda y: 0, proc_x = [proc_tok,proc_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2,\n",
       "  7,\n",
       "  1856,\n",
       "  7,\n",
       "  9851,\n",
       "  203,\n",
       "  7,\n",
       "  3612,\n",
       "  7,\n",
       "  4374,\n",
       "  221,\n",
       "  67,\n",
       "  379,\n",
       "  27,\n",
       "  2420,\n",
       "  125,\n",
       "  7,\n",
       "  181,\n",
       "  7,\n",
       "  5610,\n",
       "  2588,\n",
       "  17,\n",
       "  19,\n",
       "  1201,\n",
       "  505,\n",
       "  9,\n",
       "  7,\n",
       "  16,\n",
       "  22,\n",
       "  294,\n",
       "  335,\n",
       "  28,\n",
       "  80,\n",
       "  36114,\n",
       "  11,\n",
       "  7,\n",
       "  9851,\n",
       "  22,\n",
       "  150,\n",
       "  27,\n",
       "  7,\n",
       "  4245,\n",
       "  7,\n",
       "  17775,\n",
       "  9,\n",
       "  7,\n",
       "  152,\n",
       "  299,\n",
       "  172,\n",
       "  0,\n",
       "  14,\n",
       "  2828,\n",
       "  198,\n",
       "  74,\n",
       "  518,\n",
       "  64,\n",
       "  8,\n",
       "  29,\n",
       "  10,\n",
       "  16,\n",
       "  209,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  130,\n",
       "  28,\n",
       "  8,\n",
       "  29,\n",
       "  297,\n",
       "  1041,\n",
       "  10,\n",
       "  30,\n",
       "  42,\n",
       "  243,\n",
       "  8,\n",
       "  1473,\n",
       "  17,\n",
       "  168,\n",
       "  8,\n",
       "  31,\n",
       "  20,\n",
       "  16,\n",
       "  25,\n",
       "  37,\n",
       "  2008,\n",
       "  141,\n",
       "  26,\n",
       "  89,\n",
       "  26,\n",
       "  16,\n",
       "  95,\n",
       "  41,\n",
       "  99,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  212,\n",
       "  20,\n",
       "  16,\n",
       "  15,\n",
       "  15922,\n",
       "  47,\n",
       "  12,\n",
       "  269,\n",
       "  2715,\n",
       "  3334,\n",
       "  11,\n",
       "  675,\n",
       "  2715,\n",
       "  12056,\n",
       "  83,\n",
       "  41993,\n",
       "  200,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  29,\n",
       "  15,\n",
       "  294,\n",
       "  12,\n",
       "  0,\n",
       "  63,\n",
       "  28,\n",
       "  183,\n",
       "  69,\n",
       "  93,\n",
       "  438,\n",
       "  379,\n",
       "  47,\n",
       "  7,\n",
       "  9851,\n",
       "  10,\n",
       "  7,\n",
       "  4374,\n",
       "  10,\n",
       "  11,\n",
       "  7,\n",
       "  17775,\n",
       "  9,\n",
       "  3],\n",
       " 0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj alan xxmaj rickman & xxmaj emma xxmaj thompson give good performances with southern / xxmaj new xxmaj orleans accents in this detective flick . xxmaj it 's worth seeing for their scenes- and xxmaj rickman 's scene with xxmaj hal xxmaj holbrook . xxmaj these three actors xxunk to entertain us no matter what the movie , it seems . xxmaj the plot for the movie shows potential , but one gets the impression in watching the film that it was not pulled off as well as it could have been . xxmaj the fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things . xxmaj the movie is worth a xxunk if for nothing more than entertaining performances by xxmaj rickman , xxmaj thompson , and xxmaj holbrook . xxeos\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train.x_obj(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ld.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ld.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: LabeledData\n",
       "x: TextList (90043 items)\n",
       "[[2, 7, 1856, 7, 9851, 203, 7, 3612, 7, 4374, 221, 67, 379, 27, 2420, 125, 7, 181, 7, 5610, 2588, 17, 19, 1201, 505, 9, 7, 16, 22, 294, 335, 28, 80, 36114, 11, 7, 9851, 22, 150, 27, 7, 4245, 7, 17775, 9, 7, 152, 299, 172, 0, 14, 2828, 198, 74, 518, 64, 8, 29, 10, 16, 209, 9, 7, 8, 130, 28, 8, 29, 297, 1041, 10, 30, 42, 243, 8, 1473, 17, 168, 8, 31, 20, 16, 25, 37, 2008, 141, 26, 89, 26, 16, 95, 41, 99, 9, 7, 8, 212, 20, 16, 15, 15922, 47, 12, 269, 2715, 3334, 11, 675, 2715, 12056, 83, 41993, 200, 9, 7, 8, 29, 15, 294, 12, 0, 63, 28, 183, 69, 93, 438, 379, 47, 7, 9851, 10, 7, 4374, 10, 11, 7, 17775, 9, 3], [2, 18, 41, 131, 19, 29, 11, 18, 87, 37, 475, 28, 19, 29, 6654, 9, 18, 73, 37, 122, 59, 182, 14, 7, 1484, 107, 18, 58, 37, 53, 19, 655, 11, 112, 2011, 5759, 9, 18, 58, 37, 53, 14, 856, 702, 6654, 107, 18, 58, 37, 406, 80, 1079, 9, 7, 154, 73, 18, 159, 14, 7, 2090, 68, 18, 269, 159, 14, 7, 2495, 55, 8, 7, 2214, 7, 4947, 66, 7, 2495, 11, 8, 7, 2214, 7, 4947, 38, 8, 6237, 18, 7914, 9, 7, 684, 8, 7, 7951, 7, 3574, 58, 37, 406, 8, 7, 702, 1079, 57, 53, 88, 9, 7, 1584, 18, 105, 37, 1308, 8, 7, 2090, 1291, 74, 518, 64, 9, 18, 253, 26, 89, 1303, 14, 8, 7, 2214, 7, 4947, 11, 916, 692, 366, 11, 316, 397, 1461, 63, 54, 15, 12, 397, 16668, 9, 7, 20, 15, 44, 9, 3], [2, 7, 17, 7, 2852, 7, 3246, 10, 8, 4683, 11, 3082, 7, 5219, 7, 27581, 36, 7, 2154, 7, 7382, 33, 5103, 12, 1953, 2017, 13, 703, 23, 10521, 9560, 57, 14, 98, 214, 303, 14, 2707, 11, 11451, 17, 1685, 5488, 9, 7, 40, 4148, 11, 81, 5281, 15, 521, 11, 281, 1251, 27, 1613, 366, 9, 24, 21, 7, 23921, 21, 15, 48, 2715, 10, 1128, 11, 579, 364, 29, 59, 48, 8943, 3595, 31053, 20, 527, 12, 385, 13, 499, 55, 2193, 57, 14, 98, 214, 303, 14, 808, 4684, 11, 11451, 10, 128, 1260, 3851, 706, 14, 40, 9124, 1949, 9, 7, 17, 23482, 27, 938, 10, 19, 123, 73, 43, 8, 3068, 5611, 23, 3809, 13, 8, 2086, 7, 1204, 7, 15752, 10, 11, 454, 34, 19, 82, 10, 18, 105, 446, 133, 370, 120, 13, 40, 2887, 9, 7, 1211, 10, 63, 8, 522, 1194, 19, 779, 13, 4683, 4167, 10, 146, 141, 168, 8, 1375, 11, 4948, 7, 8362, 7, 20594, 22, 21, 7, 0, 21, 55, 7, 11038, 7, 0, 22, 21, 7, 58698, 7778, 7, 0, 7, 0, 21, 20, 15, 454, 34, 8, 136, 13, 8, 187, 567, 9, 7, 77, 1995, 15, 676, 9, 24, 7, 441, 36, 7, 3294, 33, 96, 21, 7, 23921, 537, 7, 58699, 7, 52628, 21, 36, 21, 7, 23921, 537, 7, 229, 7, 5069, 21, 33, 3], [2, 7, 19, 31, 15, 31054, 361, 27, 21, 7, 52629, 0, 3008, 7, 23922, 92, 6907, 0, 7, 0, 21, 11, 217, 124, 585, 12, 189, 14, 43, 4456, 17, 8, 116, 13, 80, 291, 9911, 9, 7, 106, 10, 217, 124, 38, 70, 480, 188, 1978, 251, 16, 267, 14, 84, 626, 64, 22, 1413, 9, 7, 348, 10, 1068, 31, 61, 2211, 11, 32, 38, 879, 14, 126, 12, 2371, 31, 188, 173, 21, 7, 10145, 7, 52630, 4674, 7, 52631, 21, 91, 329, 14, 41, 12, 146, 6384, 9, 7, 1328, 10, 18, 233, 2652, 2211, 30, 28, 8, 703, 23, 13492, 60, 54, 19, 15, 35, 12, 459, 9, 7, 152, 291, 710, 10, 216, 10, 38, 37, 8, 2048, 13, 8, 231, 31, 1198, 188, 57, 8, 7227, 5172, 128, 8027, 676, 2694, 330, 9, 24, 7, 26, 28, 8, 31, 10, 16, 22, 59, 8, 5165, 13, 7, 1656, 7, 4949, 9, 7, 19, 15, 12, 687, 459, 10, 26, 7, 1069, 7, 2481, 307, 59, 26, 94, 53, 7, 4949, 26, 7, 4269, 7, 6029, 9, 7, 17, 74, 116, 1706, 91, 39, 185, 53, 7, 4949, 9, 7, 39, 22, 1004, 8, 6966, 10, 61, 8, 378, 1132, 1404, 11, 414, 11, 15, 57, 37, 76, 520, 17, 120, 116, 36, 1718, 88, 34, 19, 10, 18, 260, 48, 7, 298, 7, 495, 1538, 11, 90, 38, 1501, 14, 140, 152, 451, 13, 200, 50, 33, 9, 7, 8, 164, 136, 7, 4949, 25, 12, 7, 2897, 7, 318, 778, 11, 591, 53, 8, 461, 34, 8, 7, 1299, 7, 974, 9682, 2297, 8635, 9, 7, 906, 10, 798, 65, 101, 306, 14, 1639, 8, 580, 28, 7, 2481, 17, 8, 3531, 15, 57, 2357, 9, 7, 133, 178, 55, 254, 61, 7, 1069, 7, 2481, 2890, 205, 52, 5173, 50, 50, 7, 39, 25, 12, 500, 306, 92, 30, 446, 37, 12, 1126, 778, 55, 5173, 1656, 9, 24, 7, 17, 1623, 14, 8, 393, 1081, 10, 7, 1656, 7, 4949, 22, 340, 25, 17, 74, 116, 53, 19, 31, 9, 7, 16, 22, 600, 20, 8, 31, 1198, 38, 177, 18021, 17, 34, 8, 945, 12976, 59, 14368, 3923, 8, 340, 13, 10465, 10, 37, 7, 4949, 9, 7, 4949, 25, 342, 17, 7, 2648, 10, 8742, 36, 37, 7, 6753, 33, 47, 12, 3946, 13237, 27, 4959, 1898, 710, 188, 37, 12, 542, 13, 347, 27, 13112, 9, 7, 216, 10, 1947, 14, 110, 7228, 10, 64, 177, 532, 7, 4949, 36, 143, 127, 1790, 330, 33, 86, 4584, 4575, 188, 49, 31055, 11, 31055, 11, 31055, 14, 8551, 12, 3821, 36, 14, 74, 10522, 33, 11, 133, 2618, 6857, 80, 930, 55, 20595, 17, 8, 1629, 9, 7, 17, 101, 686, 10, 53, 7, 740, 7, 2648, 36, 49, 25, 681, 532, 47, 2777, 20969, 68, 2072, 27, 48223, 33, 39, 1103, 706, 14, 33260, 9, 7, 17, 8, 29, 46, 207, 183, 228, 1706, 92, 101, 93, 864, 7, 1656, 7, 4949, 25, 342, 9, 24, 7, 107, 8, 31, 3254, 240, 74, 7416, 14, 164, 495, 10, 16, 22, 53, 12, 495, 2165, 26, 4161, 51, 308, 51, 175, 1369, 55, 308, 27, 12, 4959, 1229, 6629, 9, 7, 154, 37, 103, 1496, 6786, 10, 997, 3931, 11, 8, 7, 3065, 5970, 153, 32, 199, 45, 16, 66, 50, 66, 50, 7, 1195, 51, 65, 546, 137, 11, 391, 1202, 10, 107, 8, 246, 15, 2131, 5971, 628, 10, 18, 58, 35, 407, 270, 126, 16, 9, 7, 16, 22, 57, 12, 625, 11, 2131, 946, 9, 3], [2, 18, 81, 958, 34, 83, 70, 67, 124, 11, 34, 2131, 1923, 9, 7, 77, 5166, 15, 14, 363, 100, 49, 204, 14, 84, 102, 124, 14, 1156, 80, 75, 23, 11, 303, 23, 6725, 9, 24, 18, 103, 204, 14, 558, 100, 3104, 80, 75, 34, 1225, 10, 11, 204, 14, 41994, 8, 212, 20, 8, 170, 125, 1245, 13, 152, 1225, 124, 197, 35, 98, 262, 27, 16, 28, 70, 215, 9, 7, 90, 105, 186, 60, 49, 32, 38, 11, 105, 1995, 27, 60, 2030, 23, 11, 33261, 9, 24, 7, 19, 31, 705, 744, 104, 8, 1225, 2600, 9, 24, 7, 8, 170, 11, 567, 15, 7, 319, 7, 34589, 9, 7, 16, 22, 233, 12, 97, 1948, 68, 8, 567, 15, 103, 8, 170, 9, 7, 293, 39, 501, 127, 963, 34590, 9, 7, 39, 155, 35, 98, 120, 9, 7, 52, 422, 8, 412, 23, 7, 319, 34589, 9, 7, 11, 63, 32, 84, 255, 344, 47, 108, 10, 833, 16, 9, 24, 18, 492, 35, 157, 255, 59, 8, 130, 23, 415, 41, 494, 9, 18, 260, 12, 138, 3724, 47, 109, 94, 8, 170, 1194, 14, 9439, 17, 14, 8, 358, 256, 22, 411, 68, 71, 15, 2487, 11, 2150, 9, 7, 152, 215, 6508, 682, 38, 12, 138, 9683, 11, 226, 157, 158, 59, 8, 1085, 13, 351, 13, 7, 601, 7, 34589, 9, 7, 293, 39, 155, 98, 9286, 363, 9, 24, 7, 214, 494, 9, 7, 16, 22, 628, 23, 58, 35, 474, 147, 75, 34, 16, 9, 3], [2, 7, 68, 32, 185, 45, 8, 1031, 11, 370, 535, 59, 16, 48, 1105, 285, 553, 13, 29, 295, 14, 351, 93, 64, 32, 98, 149, 9, 7, 115, 193, 293, 18, 370, 8, 2675, 28, 8, 101, 29, 457, 21, 7, 0, 21, 321, 26, 54, 86, 127, 118, 13, 19, 441, 645, 59, 8, 187, 75, 27, 217, 1934, 1642, 20, 85, 1331, 780, 17, 1123, 9, 7, 216, 10, 891, 535, 59, 20, 29, 149, 18, 140, 18, 235, 19, 42, 11, 37, 20, 42, 11, 20, 29, 15, 76, 359, 64, 42, 73, 838, 12, 29, 27, 20, 441, 73, 43, 59, 9, 18, 105, 43, 1145, 10, 18, 533, 69, 13, 12, 1063, 553, 452, 11, 32, 98, 20, 17, 19, 29, 14, 65, 2555, 9, 7, 216, 10, 54, 15, 69, 535, 1170, 8, 8197, 11, 673, 1696, 26, 8, 646, 150, 13, 8, 100, 128, 596, 262, 47, 8, 14369, 45, 8, 479, 13, 8, 31, 105, 17776, 14, 9, 7, 8, 29, 103, 61, 8, 176, 779, 13, 377, 182, 1269, 46, 58, 37, 4218, 14, 41, 65, 945, 986, 10, 17, 19, 439, 16, 15, 17, 212, 12, 16093, 9, 7, 8, 101, 29, 18, 58, 37, 122, 83, 61, 20, 1331, 823, 417, 20, 5202, 232, 17, 8, 29, 11, 18, 84, 8, 586, 28, 19, 42, 15, 2007, 117, 10, 151, 16, 25, 57, 37, 8, 29, 18, 25, 1044, 9, 3], [2, 7, 44721, 2518, 66, 50, 18, 167, 774, 30, 76, 28, 8, 1029, 22, 20, 22, 57, 116, 117, 914, 14, 43, 2578, 671, 92, 7, 32, 78, 1423, 8, 231, 8, 1058, 11039, 583, 107, 16, 25, 15072, 176, 118, 11, 264, 297, 10, 30, 20, 22, 99, 244, 302, 10, 52, 8, 700, 933, 14, 43, 12, 138, 359, 1223, 10, 37, 76, 69, 9125, 50, 7, 906, 10, 8, 106, 29, 85, 8, 1270, 13, 7, 1445, 7, 5456, 23, 12, 145, 49, 95, 76, 113, 6385, 0, 17, 4685, 0, 329, 7501, 50, 23, 20, 19, 42, 6580, 1475, 10, 52, 54, 25, 74, 62, 2359, 62, 17, 255, 20, 581, 10, 16, 57, 483, 668, 9, 24, 7, 3310, 18, 81, 235, 19, 302, 68, 18, 25, 709, 10, 30, 47, 115, 494, 128, 12, 678, 355, 13, 8, 231, 18, 422, 128, 5321, 9, 7, 14, 88, 10, 54, 15, 74, 700, 14, 7, 5747, 7, 328, 10, 57, 12, 5070, 2638, 20, 91, 35, 1784, 120, 25561, 1706, 9, 3], [2, 7, 2891, 10896, 36, 27, 14925, 21, 5012, 21, 223, 0, 14782, 2510, 10, 21, 7, 2128, 21, 3718, 12, 189, 13, 4576, 14, 876, 163, 11, 105, 261, 496, 141, 110, 800, 135, 30, 8, 426, 2910, 2673, 314, 11, 7, 941, 7, 17777, 10, 49, 10322, 40, 666, 17, 240, 190, 150, 10, 103, 427, 48, 1273, 1126, 259, 9, 12, 16669, 10, 14, 43, 272, 10, 30, 8, 69, 5141, 21, 7, 1235, 21, 10, 114, 127, 174, 330, 10, 15, 12, 3891, 1518, 948, 9, 18, 1485, 32, 126, 20, 321, 9, 36, 194, 380, 125, 263, 33, 3], [2, 7, 4703, 872, 8234, 7, 25562, 7, 32173, 15, 8743, 60, 13, 287, 27, 19, 5651, 701, 274, 1654, 11, 6509, 82, 20, 2529, 51, 12, 604, 613, 13, 925, 6929, 9, 7, 32173, 209, 14, 41, 58700, 10071, 75, 11, 7255, 28, 8, 106, 213, 26, 818, 7, 941, 7, 8847, 11, 7, 16670, 7, 9492, 300, 138, 797, 104, 80, 587, 10, 4023, 430, 19642, 11, 229, 1762, 9, 24, 7, 3684, 7876, 6880, 7, 5972, 21, 7, 11452, 21, 7, 24417, 527, 26, 48, 980, 145, 34, 12, 2084, 317, 20, 289, 97, 11, 243, 40, 334, 532, 17, 8, 1629, 9, 7, 39, 3025, 51, 1093, 11, 1260, 716, 60, 14, 514, 8, 334, 13, 8, 1201, 49, 532, 40, 9, 7, 3295, 13, 1654, 38, 58701, 14, 409, 108, 51, 405, 14, 8, 365, 13, 8, 1420, 49, 61, 99, 1640, 14, 175, 1544, 30, 73, 35, 32, 140, 17, 8, 124, 485, 403, 90, 41, 7, 11452, 20970, 2030, 536, 8, 1354, 36, 49, 218, 836, 20, 652, 12, 2626, 162, 14, 8, 346, 25, 12, 498, 840, 33, 153, 12, 1062, 13, 1654, 7743, 11, 21741, 143, 64, 211, 14, 213, 9, 7, 498, 5818, 66, 7, 32, 155, 84, 16, 9, 7, 16, 22, 44, 13, 20, 11, 69, 9, 24, 7, 20596, 7, 20271, 22, 375, 179, 91, 12, 546, 317, 13, 2137, 1499, 14, 8, 11453, 30, 8, 827, 15, 25563, 11, 28315, 11, 16, 23483, 8, 31, 13, 112, 793, 11, 1071, 9, 7, 26, 7, 24417, 10, 7, 18258, 7, 6435, 15, 8, 139, 169, 17, 8, 31, 8323, 14, 7557, 102, 2450, 26, 39, 7779, 51, 3810, 1330, 14, 2548, 9, 7, 152, 10072, 1195, 7, 489, 6297, 1026, 17, 4809, 11, 2466, 251, 112, 777, 736, 9, 7, 159, 7, 872, 176, 7, 25562, 9, 3], [2, 18, 25, 879, 14, 126, 19, 31, 28, 77, 7, 195, 7, 0, 7, 15410, 731, 9, 7, 19, 31, 15, 64, 15, 378, 27, 7, 807, 512, 10, 321, 13, 10203, 60, 8, 139, 116, 60, 13, 267, 236, 55, 1134, 90, 73, 269, 3908, 59, 109, 16, 15, 308, 344, 22, 2048, 9, 7, 19, 31, 289, 163, 8, 6655, 13, 7, 20272, 10, 7, 13990, 11, 9805, 16, 1361, 1378, 34, 7, 778, 7, 28316, 9, 7, 17, 8, 1629, 13, 419, 52, 7, 1536, 289, 14, 102, 8127, 14, 113, 8, 7996, 13, 7, 778, 7, 28316, 60, 14, 43, 1816, 57, 107, 46, 38, 419, 80, 317, 17, 12, 11277, 871, 9, 7, 1536, 124, 458, 58702, 490, 8, 31, 11, 91, 37, 144, 76, 1013, 302, 63, 8, 420, 15, 128, 20597, 107, 13, 12, 32174, 58703, 9, 7, 6832, 10, 39, 133, 2533, 8, 32175, 13, 8, 10323, 757, 9, 7, 1536, 289, 14, 102, 8127, 14, 981, 1293, 717, 14, 1117, 40, 1001, 4784, 17, 19, 31, 13, 1067, 2336, 9, 3]...]\n",
       "Path: /Users/vks/.fastai/data/imdb\n",
       "y: ItemList (90043 items)\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n",
       "Path: /Users/vks/.fastai/data/imdb\n",
       "\n",
       "Valid: LabeledData\n",
       "x: TextList (9957 items)\n",
       "[[2, 7, 68, 7, 360, 15, 278, 14, 4589, 64, 48, 21, 1110, 420, 21, 15, 53, 10, 46, 1966, 52, 3284, 10, 1395, 16, 267, 1519, 686, 17, 8, 1667, 13, 8, 18445, 21, 1205, 21, 9, 24, 7, 228, 10, 120, 1205, 2301, 208, 278, 14, 11382, 17, 40, 13208, 45, 190, 1951, 9, 7, 272, 10, 46, 2791, 60, 17, 5348, 11, 1203, 1774, 537, 16, 22, 37, 53, 46, 38, 36, 55350, 4906, 33, 10226, 8945, 49, 133, 41, 12, 13285, 9, 24, 7, 906, 10, 63, 32, 38, 12, 1205, 32, 140, 44, 59, 7, 10660, 11, 7, 495, 11, 7, 2534, 11, 13, 286, 32, 199, 1368, 72, 14, 1326, 27, 1958, 717, 11, 12, 11722, 5668, 13, 111, 9, 7, 17853, 152, 200, 10, 53, 10, 44, 159, 311, 2896, 535, 10, 26902, 66, 24, 7, 906, 10, 32, 1254, 208, 27, 12, 12249, 44, 8, 75, 9, 7, 32, 38, 57, 12, 7321, 478, 13, 12, 32, 23, 140, 23, 64, 10, 20, 22, 109, 16, 15, 10, 30714, 44, 9, 24, 7, 11, 13, 286, 32, 3529, 10, 53, 308, 49, 133, 17956, 178, 10, 30, 32, 3529, 17853, 16, 22, 53, 605, 2896, 535, 10, 26902, 9, 7, 11, 32, 199, 285, 9, 7, 20, 15, 2761, 9, 24, 7, 11, 13, 286, 32, 78, 547, 537, 32, 199, 12, 5928, 9, 12, 5928, 49, 670, 75, 14, 1997, 0, 1209, 1955, 39, 91, 35, 5035, 23558, 9, 7, 11, 1955, 39, 91, 35, 3529, 55, 2707, 3165, 107, 39, 1116, 12, 6984, 3380, 37234, 9, 24, 7, 11, 32, 1326, 12, 28033, 1106, 1351, 537, 7, 11159, 7, 2110, 9, 7, 89, 10, 18, 492, 35, 76, 958, 7, 2154, 7, 7113, 9, 7, 735, 7, 807, 61, 623, 8, 4227, 34, 8, 431, 494, 9, 24, 7, 19, 29, 15, 12, 22022, 13, 12, 7, 17692, 203, 7, 21546, 553, 1351, 36, 17, 101, 686, 6469, 1378, 13, 111, 33, 96, 21, 7, 1158, 10, 20, 22, 64, 18, 73, 43, 53, 63, 18, 25, 12, 1205, 9, 21, 7, 30, 393, 100, 11, 393, 7854, 17, 19, 439, 78, 37, 838, 8, 477, 13, 9494, 9, 3], [2, 7, 271, 29, 144, 131, 9, 7, 271, 137, 117, 9, 18, 78, 37, 838, 12, 29, 460, 115, 19, 9, 7, 183, 14, 84, 9, 7, 74, 137, 45, 44, 9, 2326, 1333, 36, 172, 33, 155, 185, 28, 175, 317, 9, 18, 0, 406, 49, 25, 393, 214, 14, 177, 300, 303, 104, 19, 29, 9, 24, 18, 167, 774, 28, 7, 2145, 7, 2690, 9, 7, 225, 43, 1190, 92, 18, 78, 37, 838, 109, 0, 40, 19100, 225, 43, 14, 4361, 652, 8, 317, 50, 24, 7, 8, 2139, 17, 8, 29, 92, 398, 46, 146, 1303, 14, 12438, 9, 24, 7, 26, 28, 8, 995, 145, 92, 64, 12, 473, 50, 7, 39, 146, 43, 300, 34, 12, 473, 11, 792, 54, 50, 18, 78, 84, 108, 128, 69, 1150, 45, 12597, 269, 93, 137, 9, 24, 7, 464, 707, 96, 7, 58, 37, 822, 92, 58, 37, 808, 50, 3], [2, 7, 89, 16, 15, 59, 11664, 174, 17, 8, 712, 11, 90, 41, 455, 34305, 3578, 8, 3846, 3791, 223, 36451, 50, 50, 7, 30, 1089, 90, 151, 376, 1830, 20, 1212, 3858, 10, 331, 347, 38, 151, 2571, 273, 101, 974, 10, 11, 405, 323, 10, 7354, 10, 997, 530, 402, 7, 1123, 2492, 2542, 331, 347, 38, 151, 405, 8, 362, 148, 13, 8, 1303, 50, 7, 6344, 17, 2426, 331, 4254, 3777, 20, 185, 53, 46, 199, 51, 7, 14797, 1535, 520, 8212, 27, 8, 32336, 11, 890, 9, 7, 7392, 100, 151, 5034, 43255, 14, 840, 10, 74, 4032, 26672, 10, 19678, 10, 55, 9100, 3218, 9, 7, 1884, 151, 307, 11, 1399, 802, 9, 7, 381, 18, 159, 34, 92, 7, 17, 362, 19, 840, 25, 342, 34, 12, 760, 843, 23, 837, 288, 382, 356, 3912, 10, 172, 10, 11, 74, 164, 326, 304, 14, 1109, 13, 9, 7, 8, 479, 10, 8, 659, 10, 11, 8, 290, 25, 1154, 9, 18, 41, 14, 159, 141, 11, 1212, 563, 166, 54, 15, 183, 337, 14, 432, 28, 9, 3], [2, 12, 3432, 13, 14113, 210, 1106, 7266, 2239, 1152, 14, 159, 8395, 27, 12, 2114, 9, 12, 1334, 27736, 204, 14, 3560, 111, 14, 3727, 8, 4183, 234, 8, 336, 2893, 10, 46, 103, 41, 14, 11507, 27, 8200, 10, 48, 7, 1318, 11, 12, 11812, 6423, 783, 553, 169, 9, 7, 294, 168, 28, 81, 388, 999, 10, 7, 740, 62, 7, 3169, 62, 7, 5950, 36, 12, 1089, 13737, 627, 23, 29, 7798, 33, 15, 34, 517, 26, 12, 16299, 11, 8, 101, 263, 4218, 14, 8, 1503, 7, 15800, 36, 17, 42, 13, 81, 388, 703, 23, 1460, 587, 71, 85, 33, 9, 7, 217, 41, 70, 413, 587, 9, 7, 117, 97, 296, 344, 17, 8, 29, 15, 13751, 97, 9, 24, 7, 77, 7, 1232, 96, 11464, 24, 7, 40577, 291, 7, 2334, 96, 7, 231, 7, 1594, 24, 7, 772, 7, 1862, 96, 224, 14659, 13, 3558, 10, 263, 11204, 3], [2, 18, 1601, 19, 556, 16, 73, 43, 206, 67, 57, 47, 8, 1031, 13, 8, 29, 439, 9, 7, 1650, 11, 7, 6445, 679, 60, 206, 67, 489, 3194, 8, 145, 49, 532, 40, 334, 34, 12, 6201, 27, 12, 605, 923, 10, 30, 19, 29, 207, 11640, 14136, 26, 16, 434, 34, 9, 7, 599, 7, 4458, 15, 1166, 306, 282, 68, 39, 310, 12, 232, 53, 19, 117, 97, 8, 29, 25, 12, 449, 13, 628, 16, 83, 1022, 40, 640, 9, 7, 1650, 11, 7, 6445, 25, 89, 1055, 1011, 18, 538, 16, 12, 388, 155, 41, 538, 16, 12, 263, 10, 18, 538, 16, 48, 1580, 339, 57, 107, 7, 599, 7, 4458, 22, 923, 25, 605, 9, 3], [2, 194, 194, 7, 634, 7, 1061, 7, 1429, 194, 194, 24, 7, 1955, 467, 13, 97, 118, 55284, 28, 69, 93, 12, 191, 249, 10, 12, 412, 20, 10190, 295, 72, 15, 20, 13, 7, 2527, 7, 14038, 9, 7, 19, 524, 170, 61, 384, 198, 5707, 13, 101, 15160, 36, 0, 8, 169, 51, 9447, 33, 10, 697, 23, 691, 764, 3317, 53, 12, 3821, 28, 206, 421, 10, 11, 16489, 3593, 1434, 53, 202, 34, 198, 11, 6284, 4574, 1493, 9, 8, 11812, 6423, 201, 15, 12, 0, 14, 157, 8, 242, 9, 7, 15182, 8, 212, 20, 7, 11812, 7, 6423, 15, 579, 215, 11, 6056, 10, 7, 2527, 757, 19, 21568, 34, 12, 1953, 11, 2093, 7, 2499, 2199, 9, 7, 433, 34, 10, 8, 31, 6143, 65, 7099, 36, 28, 8, 356, 33, 5331, 1370, 11, 2254, 65, 881, 17, 2763, 13, 416, 9, 7, 115, 16, 13043, 104, 20862, 426, 11122, 47, 74, 23, 412, 172, 27, 41124, 7, 5547, 2588, 10, 37, 14, 767, 12, 784, 20, 105, 58, 183, 28, 8, 94, 23, 14173, 55245, 9, 7, 45, 42, 241, 10, 11663, 2080, 21, 7, 32, 7, 213, 7, 8, 7, 323, 7, 1187, 10, 18, 257, 7, 213, 7, 8, 7, 382, 7, 1187, 10, 21, 57, 14, 1364, 17, 42, 69, 7, 5547, 4598, 9, 7, 63, 7, 5547, 100, 144, 1152, 14, 1902, 34, 8, 7, 1001, 7, 8364, 15156, 46, 257, 4838, 7, 2527, 7, 14038, 143, 19, 31, 10, 40, 20158, 10156, 9, 7, 8, 783, 307, 53, 12, 1334, 24630, 23, 18416, 3512, 11, 16, 180, 8, 2795, 17, 7, 0, 203, 7, 9903, 185, 3680, 47, 1875, 9, 7, 17, 42, 3348, 150, 7, 30141, 324, 14, 1298, 11, 10, 14, 17354, 65, 34138, 1305, 10, 8, 4923, 23, 2564, 215, 13992, 515, 14, 2282, 536, 12, 2651, 23, 11, 8, 1305, 58, 35, 84, 16, 50, 8, 11812, 6423, 201, 15, 12, 314, 351, 23, 0, 20, 225, 43, 131, 23, 458, 236, 188, 14, 43, 2463, 9, 3], [2, 7, 261, 8, 1168, 169, 59, 7, 1322, 7, 7862, 20, 4764, 16, 8, 110, 15, 8, 576, 9, 7, 39, 22, 207, 3669, 429, 44, 143, 108, 10, 30, 20, 91, 35, 558, 108, 51, 21, 405, 8, 256, 9, 21, 7, 261, 42, 13, 8, 195, 22, 271, 1248, 9448, 5936, 10, 39, 1108, 14, 558, 4763, 119, 39, 5083, 40, 531, 9, 7, 89, 10, 40, 181, 317, 15, 4763, 151, 10, 26, 12, 756, 13, 6176, 2160, 1203, 108, 14, 1241, 80, 11919, 531, 26, 46, 3491, 3000, 9, 7, 54, 22, 183, 83, 14, 20957, 32, 14, 7, 5006, 10, 76, 8, 886, 39, 22, 1338, 104, 15, 206, 393, 9, 7, 17, 8, 148, 10, 45, 8, 12878, 150, 10, 32, 257, 662, 20, 7, 5006, 243, 532, 27, 44, 8, 8200, 9, 7, 98, 19, 96, 7, 39, 22, 342, 1566, 10, 302, 17, 8, 3090, 11, 302, 17, 8, 3800, 10, 11, 151, 1009, 14, 7471, 72, 8, 6046, 12, 138, 9, 7, 63, 81, 7, 0, 85, 146, 5166, 92, 24, 7, 782, 19, 42, 929, 32, 199, 168, 8, 3966, 2611, 353, 9, 3], [2, 7, 63, 46, 85, 12, 7, 1737, 60, 13, 184, 18, 73, 13, 6079, 16, 9, 7, 312, 588, 17, 19, 31, 155, 43, 3052, 13, 529, 652, 303, 51, 8, 1030, 9, 18, 58, 35, 140, 109, 124, 53, 19, 98, 645, 7, 397, 55, 7, 963, 7, 1207, 9, 18, 260, 688, 17, 7, 2540, 7, 20404, 9, 7, 6513, 7, 8714, 25, 17, 16, 28, 59, 743, 249, 10, 52, 18, 197, 35, 98, 20, 1176, 45, 108, 9, 7, 81, 8, 420, 49, 3860, 108, 14, 43, 8, 339, 17, 16, 9, 7, 16, 15, 53, 7, 7718, 7, 4579, 17, 7, 2508, 9, 7, 54, 15, 74, 130, 573, 7, 20404, 2781, 11, 1395, 40, 442, 17, 8, 21656, 9, 7, 154, 46, 86, 2781, 28, 8, 6306, 13, 584, 81, 8, 170, 695, 9, 7, 46, 155, 13, 789, 19, 31, 17, 7, 10841, 11, 4121, 16, 28, 1005, 14, 409, 44, 8, 4614, 2334, 2317, 28, 8, 328, 9, 7, 54, 15, 183, 69, 14, 157, 59, 19, 31, 20, 44, 8, 101, 2063, 41, 429, 9, 18, 662, 18, 95, 833, 19, 29, 16, 4764, 77, 1229, 9, 3], [2, 0, 15, 12, 7, 389, 7, 1899, 391, 9, 18, 699, 64, 14, 533, 36, 70, 138, 504, 33, 30, 18, 87, 35, 533, 19, 14, 43, 26, 1357, 26, 0, 25, 14, 126, 9, 24, 7, 16, 22, 57, 400, 9, 7, 97, 137, 10, 1479, 246, 11, 470, 9, 7, 642, 121, 18, 486, 14, 514, 9, 7, 8, 239, 169, 25, 261, 114, 17, 42, 1264, 9, 18, 161, 131, 763, 13, 9473, 20, 86, 69, 1319, 11, 1126, 93, 19, 18066, 9, 24, 7, 16, 61, 8, 185, 11, 248, 13, 48, 11887, 440, 13, 12, 952, 4358, 264, 227, 74, 42, 61, 144, 131, 9, 7, 16, 25, 12, 9016, 14, 126, 11, 18, 95, 248, 77, 351, 405, 46453, 11, 46453, 47, 8, 878, 9, 7, 168, 12, 29, 155, 35, 43, 19, 94, 267, 179, 9, 3], [2, 7, 19, 61, 207, 14, 43, 8, 271, 439, 13, 143, 137, 254, 8, 1173, 960, 9, 7, 37, 57, 42, 55, 127, 172, 30, 2257, 8, 465, 196, 9, 7, 852, 7, 13223, 11, 7, 4932, 7, 19549, 86, 500, 30, 8, 396, 13, 111, 185, 53, 80, 106, 75, 137, 9, 24, 7, 52, 8, 356, 25, 37, 3650, 9, 7, 94, 13, 8, 1366, 11, 288, 86, 842, 30, 54, 86, 129, 200, 20, 1902, 60, 45, 8, 522, 14, 301, 198, 140, 20, 46, 95, 35, 1371, 831, 55, 98, 44, 13, 8, 3912, 14, 1169, 8, 824, 9, 24, 18, 197, 35, 122, 13, 42, 1285, 13, 8, 31, 20, 18, 447, 55, 87, 35, 4597, 77, 431, 9, 7, 147, 75, 105, 43, 146, 1039, 3293, 29716, 10428, 51, 147, 0, 9, 3]...]\n",
       "Path: /Users/vks/.fastai/data/imdb\n",
       "y: ItemList (9957 items)\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n",
       "Path: /Users/vks/.fastai/data/imdb\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modelling Databunch: Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bit of work to convert our LabelList in a DataBunch as we don't just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the newt words in the text. All of this is done with the next object called LM_PreLoader. At the beginning of each epoch, it'll shuffle the articles (if shuffle=True) and create a big stream by concatenating all of them. We divide this big stream in bs smaller streams. That we will read in chunks of bptt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Just using those for illustration purposes, they're not used otherwise.\n",
    "from IPython.display import display,HTML\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we split it in 6 batches it would give something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = \"\"\"\n",
    "In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. \n",
    "First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the Processor used in the data block API.\n",
    "Then we will study how we build a language model and train it.\\n\n",
    "\"\"\"\n",
    "tokens = np.array(tp([stream])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we have a bptt of 5, we would go over those three batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs,seq_len = 6,15\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we have a bptt of 5, we would go over those three batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs,bptt = 6,5\n",
    "for k in range(3):\n",
    "    d_tokens = np.array([tokens[i*seq_len + k*bptt:i*seq_len + (k+1)*bptt] for i in range(bs)])\n",
    "    df = pd.DataFrame(d_tokens)\n",
    "    display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LM_PreLoader():\n",
    "    \n",
    "    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "        self.data, self.bs, self.bptt, self.shuffle = data, bs, bptt, shuffle\n",
    "        total_len = sum([len(x) for x in data.x])\n",
    "        self.n_batch = total_len // bs\n",
    "        self.batchify()\n",
    "        \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched_data[idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1]\n",
    "        \n",
    "    def batchify(self):\n",
    "        texts = self.data.x\n",
    "        if self.shuffle: texts = texts[torch.randperm(len(texts))]\n",
    "        stream = torch.cat([tensor(t) for t in texts])\n",
    "        self.batched_data = stream[:self.n_batch*self.bs].view(self.bs, self.n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(LM_PreLoader(ll.valid, shuffle=True), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)\n",
    "x1,y1 = next(iter_dl)\n",
    "x2,y2 = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(),y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = proc_num.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj pleasantly surprised by xxmaj tyra 's bubbly rendition of a doll 's outlook and reaction to ' real life ' . xxmaj the situation of child angst and need for her mother can be handled seriously or lightly . xxmaj both help get the point across that in tough times , children need love and support . xxmaj so what if it comes from a doll accidentally brought\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxmaj pleasantly surprised by xxmaj tyra 's bubbly rendition of a doll 's outlook and reaction to ' real life ' . xxmaj the situation of child angst and need for her mother can be handled seriously or lightly . xxmaj both help get the point across that in tough times , children need love and support . xxmaj so what if it comes from a doll accidentally brought to\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in y1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to life . xxmaj the movie did not go too overboard on needing to correct the situation as other movies have . xxmaj the doll 's outlook on life based on infusion from commercial ditties and tv perfection helps to show the difference between tv and reality in a way kids can understand . xxmaj talk to you kids about this after the movie but enjoy xxmaj eve 's bubbly\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n",
    "            DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))\n",
    "\n",
    "def lm_databunchify(sd, bs, bptt, **kwargs):\n",
    "    return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lib.nb_04.DataBunch at 0x1a2b13d128>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification DataBunch: Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_cat = CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:58<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 01:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test'])\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxbos xxmaj well ... tremors i , the original started off in 1990 and i found the movie quite enjoyable to watch . however , they proceeded to make tremors ii and iii . xxmaj trust me , those movies started going downhill right after they finished the first one , i mean , ass blasters ? ? ? xxmaj now , only xxmaj god himself is capable of answering the question \" why in xxmaj gods name would they create another one of these dumpster dives of a movie ? \" xxmaj tremors iv can not be considered a bad movie , in fact it can not be even considered an epitome of a bad movie , for it lives up to more than that . xxmaj as i attempted to sit though it , i noticed that my eyes started to bleed , and i hoped profusely that the little girl from the ring would crawl through the tv and kill me . did they really think that dressing the people who had stared in the other movies up as though they we \\'re from the wild west would make the movie ( with the exact same occurrences ) any better ? honestly , i would never suggest buying this movie , i mean , there are cheaper ways to find things that burn well . xxeos',\n",
       "  'neg'),\n",
       " (\"xxbos i saw this film a while back and it 's still at the top of my ' favorite movies ' list . xxmaj it is amazingly put together and what really makes the film are the detailed tid bits ( such as the ' xxmaj cafe xxmaj xxunk ' coffee crate being reused as a cup to wash her grandsons hair ) that people are n't seeing because you will not understand this movie unless you are hispanic . xxmaj this is just one of those films that is very culturally specific and particular . xxmaj please do not bash this film if you have no prior knowledge of what foundation it 's being built upon . i completely see what the writer / director was going for , and he hit the target perfectly ! xxmaj this film is highly deserving of a better rating . xxeos\",\n",
       "  'pos')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1,12552]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_cat.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation set, we will simply sort the samples by length, and we begin with the longest ones for memory reasons (it's better to always have the biggest tensors first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SortSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, data_source, key):\n",
    "        self.data_source, self.key = data_source, key\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size 50 * bs. We sort those megabatches by length before splitting them in 50 minibatches. That way we will have randomized batches of roughly the same length.\n",
    "\n",
    "Then we make sure to have the biggest batch first and shuffle the order of the other batches. We also make sure the last batch stays at the end because its size is probably lower than batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source,self.key,self.bs = data_source,key,bs\n",
    "\n",
    "    def __len__(self) -> int: return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]\n",
    "        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2)\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding: we had the padding token (that as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use PyTorch convenience functions that will let us ignore that padding (see 12c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = torch.LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = torch.LongTensor(s[0])\n",
    "    return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs)\n",
    "train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(train_dl)\n",
    "x,y = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3311, 2425, 1621, 1577, 1481], 1001)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([312, 312, 312, 311, 311], 299)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter_dl)\n",
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   18,   25,  ...,  156,    9,    3],\n",
       "        [   2,    7, 1522,  ...,   19,   29,    3],\n",
       "        [   2, 1287, 1061,  ...,  136,    9,    3],\n",
       "        ...,\n",
       "        [   2,    7,  106,  ...,    1,    1,    1],\n",
       "        [   2,   18,   85,  ...,    1,    1,    1],\n",
       "        [   2,   18,  699,  ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))\n",
    "\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "    return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = clas_databunchify(ll, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 05_datablocks_text.ipynb to lib/nb_05.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 05_datablocks_text.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-fastai",
   "language": "python",
   "name": "my-fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
